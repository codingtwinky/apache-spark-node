- figure out why logging all the INFO/WARN crap
- figure out tests
- instead of "fake presence of a main class so that SparkSubmitArguments
  doesn't bail", we should build a stub class and load that one, so that we
  don't get the warning that the jar doesn't exist
- write up README
  - intro/overview
  - mimic http://spark.apache.org/docs/latest/sql-programming-guide.html in js




DONE

- Look into how pyspark (and scala and maybe java?) handle dataframe
  expressions.
  Python uses operator overloading e.g. __getitem__ is defined on
  Dataframe such that df['name'] returns a Column, and then things like
  __add__ are defined on Column such that a you can do df['name'] + 1.

  But all ops are available as regular functions, so we can start off with
  that. e.g the example that is done as `df.filter(df("age") > 21).show()` we
  can do as `def.filter("age > 21")` (or pedantically,
  `df.filter(df.col("age").gt(21))`).

  (Of course this is to get started; we could later add tooling to allow the
  infix operator syntax like in python and scala).

